
<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>SSL Datasets</title>
        <link
            href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"
            rel="stylesheet"
            integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN"
            crossorigin="anonymous">
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <nav
            class="navbar navbar-expand-sm navbar-light justify-content-between">
            <a class="nav-brand my-1 ms-1"
                href="https://neurostatslab.org/"><img
                    src="res/NSL_Logo_RGB_Black.png" height="50" /></a>
        </nav>

        <div class="container-fluid" id="top-level">
            <div class="container round bg-light" id="header-container">
                <div class="row justify-content-center">
                    <div class="col-12">
                        <h1 id="heading"
                            class="text-dark display-3 text-center">Vocal Call
                            Locator Benchmark (VCL) for Localizing Rodent
                            Vocalizations From Multichannel Audio</h1>
                    </div>
                    <div class="col-10" id="author-list">
                        <h3 class="text-dark text-center">
                            Ralph E Peterson<sup>1,2&#42;</sup>,
                            Aramis Tanelus<sup>2&#42;</sup>,
                            Christopher Ick<sup>3</sup>,
                            Bartul Mimica<sup>4</sup>,
                            Niegil Francis<sup>1,5</sup>,
                            Violet J Ivan<sup>1</sup>,
                            Aman Choudhri<sup>6</sup>,
                            Annegret L Falkner<sup>4</sup>,
                            Mala Murthy<sup>4</sup>,
                            David M Schneider<sup>1</sup>,
                            Dan H Sanes<sup>1</sup>,
                            Alex H Williams<sup>1,2&#134;</sup>
                        </h3>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12">
                        <h6 class="text-dark text-center">
                            <sup>1</sup>NYU, Center for Neural Science<br>
                            <sup>2</sup>Flatiron Institute, Center for
                            Computational Neuroscience<br>
                            <sup>3</sup>NYU, Center for Data Science<br>
                            <sup>4</sup>Princeton Neuroscience Institute<br>
                            <sup>5</sup>NYU, Tandon School of Engineering<br>
                            <sup>6</sup>Columbia University<br>
                        </h3>
                    </div>
                </div>

                <!-- small spacer -->
                <div class="row justify-content-center">
                    <div class="col-12 justify-content-center"
                        style="height: 4em">
                    </div>
                </div>

                <div class="row">
                    <div class="col-12">
                        <h5 class="text-dark text-center">Abstract</h5>
                    </div>
                </div>
                <div class="row justify-content-center">
                    <div class="col-sm-10 col-6">
                        <p class='text-muted'>
                            Understanding the behavioral and neural dynamics of
                            social interactions is a goal of contemporary
                            neuroscience. Many machine learning methods have
                            emerged in recent years to make sense of complex
                            video and neurophysiological data that result from
                            these experiments. Less focus has been placed on
                            understanding how animals process acoustic
                            information, including social vocalizations. A
                            critical step to bridge this gap is determining the
                            senders and receivers of acoustic information in
                            social interactions. While sound source localization
                            (SSL) is a classic problem in signal processing,
                            existing approaches are limited in their ability to
                            localize animal-generated sounds in standard
                            laboratory environments. Advances in deep learning
                            methods for SSL are likely to help address these
                            limitations, however there are currently no publicly
                            available models, datasets, or benchmarks to
                            systematically evaluate SSL algorithms in the domain
                            of bioacoustics. Here, we present the VCL Benchmark:
                            the first large-scale dataset for benchmarking SSL
                            algorithms in rodents. We acquired synchronized
                            video and multi-channel audio recordings of 767,295
                            sounds with annotated ground truth sources across 9
                            conditions. The dataset provides benchmarks which
                            evaluate SSL performance on real data, simulated
                            acoustic data, and a mixture of real and simulated
                            data. We intend for this benchmark to facilitate
                            knowledge transfer between the neuroscience and
                            acoustic machine learning communities, which have
                            had limited overlap.
                        </p>
                    </div>
                </div>
                <div class="row justify-content-center">
                    <div class="col-sm-3 col-md-1">
                        <a
                            href="https://www.biorxiv.org/content/10.1101/2024.09.20.613758v1"><img
                                src="res/arxiv.svg" height="50" /></a>
                    </div>
                    <div class="col-sm-3 col-md-1">
                        <a
                            href="https://github.com/neurostatslab/vocalocator/"><img
                                src="res/github-mark.svg" height="50" /></a>
                    </div>
                </div>

                <div class="row justify-content-center">
                    <div class="col-11">
                        <p class="text-muted">
                            <sup>&#42;</sup>Equal contribution<br>
                            <sup>&#134;</sup>Correspondence to
                            rep359@nyu.edu and alex.h.williams@nyu.edu
                        </p>
                    </div>
                </div>

                <!-- small spacer -->
                <div class="row justify-content-center">
                    <div class="col-12 justify-content-center"
                        style="height: 4em">
                        <hr>
                    </div>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 justify-content-center">
                        <h2 class="text-dark text-center">Datasets</h2>
                    </div>
                </div>

                <!-- MARK: dataset card container-->
                <div class="container" id="datasets-container">
                    <div class="row mt-3">
                        {dataset_cards}
                    </div>
                </div>

                <!--  another small spacer -->
                <div class="row justify-content-center">
                    <div class="col-12 justify-content-center"
                        style="height: 4em">
                        <hr>
                    </div>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 justify-content-center">
                        <h2 class="text-dark text-center">Environments</h2>
                    </div>
                </div>

                <!-- MARK: environment card container -->
                <div class="container" id="environments-container">
                    <div class="row mt-3">
                        {environment_cards}
                    </div>
                </div>
                <!-- End: environment card container -->

            </div>
        </div>

        <!-- MARK: details modals (hidden)-->
        {dataset_details_modals}
        <!-- End: dataset details modals-->

        <!-- MARK: env details modals (hidden)-->
        {environment_details_modals}
        <!-- End: env details modals-->

        <!-- MARK: download modals (hidden)-->
        {download_modals}
        <!-- End: download modals-->

        <!-- Footer with MIT license info -->
        <div class="container" id="footer-container">
            <footer>
                <div class="row">
                    <div class="col-12">
                        <p class="text-muted">
                            <a
                                href="https://github.com/neurostatslab/gerbilizer/blob/main/LICENSE">MIT
                                License</a> | &copy; 2024 NeuroStats Lab
                        </p>
                    </div>
                </div>
            </footer>
        </div>
        <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
            integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
            crossorigin="anonymous"></script>
        <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"
            integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
            crossorigin="anonymous"></script>
        <script
            src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js"
            integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
            crossorigin="anonymous"></script>
    </body>
</html>
